{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362c5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#Needed for nltk.pos_tag function nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e85c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cd7682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shahv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shahv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0759dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Welcome to Wikipedia! As a new editor, you will become a Wikipedian. Before starting a new article, please review Wikipedia's notability requirements. In short, the topic of an article must have already been the subject of publication in reliable, secondary, entirely independent sources that treat the topic in substantive detail, such as books, newspapers, magazines, peer-reviewed scholarly journals and websites that meet the same requirements as reputable print-based sources. Information on Wikipedia must be verifiable; if no reliable third-party sources can be found on a topic, then it should not have a separate article. Please search Wikipedia first to make sure that an article does not already exist on the subject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acba07bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Welcome to Wikipedia! As a new editor, you will become a Wikipedian. Before starting a new article, please review Wikipedia's notability requirements. In short, the topic of an article must have already been the subject of publication in reliable, secondary, entirely independent sources that treat the topic in substantive detail, such as books, newspapers, magazines, peer-reviewed scholarly journals and websites that meet the same requirements as reputable print-based sources. Information on Wikipedia must be verifiable; if no reliable third-party sources can be found on a topic, then it should not have a separate article. Please search Wikipedia first to make sure that an article does not already exist on the subject\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f65dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'Wikipedia',\n",
       " '!',\n",
       " 'As',\n",
       " 'a',\n",
       " 'new',\n",
       " 'editor',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'become',\n",
       " 'a',\n",
       " 'Wikipedian',\n",
       " '.',\n",
       " 'Before',\n",
       " 'starting',\n",
       " 'a',\n",
       " 'new',\n",
       " 'article',\n",
       " ',',\n",
       " 'please',\n",
       " 'review',\n",
       " 'Wikipedia',\n",
       " \"'s\",\n",
       " 'notability',\n",
       " 'requirements',\n",
       " '.',\n",
       " 'In',\n",
       " 'short',\n",
       " ',',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'of',\n",
       " 'an',\n",
       " 'article',\n",
       " 'must',\n",
       " 'have',\n",
       " 'already',\n",
       " 'been',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'publication',\n",
       " 'in',\n",
       " 'reliable',\n",
       " ',',\n",
       " 'secondary',\n",
       " ',',\n",
       " 'entirely',\n",
       " 'independent',\n",
       " 'sources',\n",
       " 'that',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'in',\n",
       " 'substantive',\n",
       " 'detail',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'books',\n",
       " ',',\n",
       " 'newspapers',\n",
       " ',',\n",
       " 'magazines',\n",
       " ',',\n",
       " 'peer-reviewed',\n",
       " 'scholarly',\n",
       " 'journals',\n",
       " 'and',\n",
       " 'websites',\n",
       " 'that',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'same',\n",
       " 'requirements',\n",
       " 'as',\n",
       " 'reputable',\n",
       " 'print-based',\n",
       " 'sources',\n",
       " '.',\n",
       " 'Information',\n",
       " 'on',\n",
       " 'Wikipedia',\n",
       " 'must',\n",
       " 'be',\n",
       " 'verifiable',\n",
       " ';',\n",
       " 'if',\n",
       " 'no',\n",
       " 'reliable',\n",
       " 'third-party',\n",
       " 'sources',\n",
       " 'can',\n",
       " 'be',\n",
       " 'found',\n",
       " 'on',\n",
       " 'a',\n",
       " 'topic',\n",
       " ',',\n",
       " 'then',\n",
       " 'it',\n",
       " 'should',\n",
       " 'not',\n",
       " 'have',\n",
       " 'a',\n",
       " 'separate',\n",
       " 'article',\n",
       " '.',\n",
       " 'Please',\n",
       " 'search',\n",
       " 'Wikipedia',\n",
       " 'first',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'an',\n",
       " 'article',\n",
       " 'does',\n",
       " 'not',\n",
       " 'already',\n",
       " 'exist',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a98fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to Wikipedia!',\n",
       " 'As a new editor, you will become a Wikipedian.',\n",
       " \"Before starting a new article, please review Wikipedia's notability requirements.\",\n",
       " 'In short, the topic of an article must have already been the subject of publication in reliable, secondary, entirely independent sources that treat the topic in substantive detail, such as books, newspapers, magazines, peer-reviewed scholarly journals and websites that meet the same requirements as reputable print-based sources.',\n",
       " 'Information on Wikipedia must be verifiable; if no reliable third-party sources can be found on a topic, then it should not have a separate article.',\n",
       " 'Please search Wikipedia first to make sure that an article does not already exist on the subject']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2711953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'to',\n",
       " 'wikipedia',\n",
       " '!',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'editor',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'become',\n",
       " 'a',\n",
       " 'wikipedian',\n",
       " '.',\n",
       " 'before',\n",
       " 'starting',\n",
       " 'a',\n",
       " 'new',\n",
       " 'article',\n",
       " ',',\n",
       " 'please',\n",
       " 'review',\n",
       " 'wikipedia',\n",
       " \"'s\",\n",
       " 'notability',\n",
       " 'requirements',\n",
       " '.',\n",
       " 'in',\n",
       " 'short',\n",
       " ',',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'of',\n",
       " 'an',\n",
       " 'article',\n",
       " 'must',\n",
       " 'have',\n",
       " 'already',\n",
       " 'been',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'publication',\n",
       " 'in',\n",
       " 'reliable',\n",
       " ',',\n",
       " 'secondary',\n",
       " ',',\n",
       " 'entirely',\n",
       " 'independent',\n",
       " 'sources',\n",
       " 'that',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'in',\n",
       " 'substantive',\n",
       " 'detail',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'books',\n",
       " ',',\n",
       " 'newspapers',\n",
       " ',',\n",
       " 'magazines',\n",
       " ',',\n",
       " 'peer-reviewed',\n",
       " 'scholarly',\n",
       " 'journals',\n",
       " 'and',\n",
       " 'websites',\n",
       " 'that',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'same',\n",
       " 'requirements',\n",
       " 'as',\n",
       " 'reputable',\n",
       " 'print-based',\n",
       " 'sources',\n",
       " '.',\n",
       " 'information',\n",
       " 'on',\n",
       " 'wikipedia',\n",
       " 'must',\n",
       " 'be',\n",
       " 'verifiable',\n",
       " ';',\n",
       " 'if',\n",
       " 'no',\n",
       " 'reliable',\n",
       " 'third-party',\n",
       " 'sources',\n",
       " 'can',\n",
       " 'be',\n",
       " 'found',\n",
       " 'on',\n",
       " 'a',\n",
       " 'topic',\n",
       " ',',\n",
       " 'then',\n",
       " 'it',\n",
       " 'should',\n",
       " 'not',\n",
       " 'have',\n",
       " 'a',\n",
       " 'separate',\n",
       " 'article',\n",
       " '.',\n",
       " 'please',\n",
       " 'search',\n",
       " 'wikipedia',\n",
       " 'first',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'an',\n",
       " 'article',\n",
       " 'does',\n",
       " 'not',\n",
       " 'already',\n",
       " 'exist',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lowercasing\n",
    "lowercase_tokens = []\n",
    "for token in tokens:\n",
    "    lowercase_tokens.append(token.lower())\n",
    "lowercase_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291ae75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Welcome', 'to')\n",
      "('to', 'Wikipedia!')\n",
      "('Wikipedia!', 'As')\n",
      "('As', 'a')\n",
      "('a', 'new')\n",
      "('new', 'editor,')\n",
      "('editor,', 'you')\n",
      "('you', 'will')\n",
      "('will', 'become')\n",
      "('become', 'a')\n",
      "('a', 'Wikipedian.')\n",
      "('Wikipedian.', 'Before')\n",
      "('Before', 'starting')\n",
      "('starting', 'a')\n",
      "('a', 'new')\n",
      "('new', 'article,')\n",
      "('article,', 'please')\n",
      "('please', 'review')\n",
      "('review', \"Wikipedia's\")\n",
      "(\"Wikipedia's\", 'notability')\n",
      "('notability', 'requirements.')\n",
      "('requirements.', 'In')\n",
      "('In', 'short,')\n",
      "('short,', 'the')\n",
      "('the', 'topic')\n",
      "('topic', 'of')\n",
      "('of', 'an')\n",
      "('an', 'article')\n",
      "('article', 'must')\n",
      "('must', 'have')\n",
      "('have', 'already')\n",
      "('already', 'been')\n",
      "('been', 'the')\n",
      "('the', 'subject')\n",
      "('subject', 'of')\n",
      "('of', 'publication')\n",
      "('publication', 'in')\n",
      "('in', 'reliable,')\n",
      "('reliable,', 'secondary,')\n",
      "('secondary,', 'entirely')\n",
      "('entirely', 'independent')\n",
      "('independent', 'sources')\n",
      "('sources', 'that')\n",
      "('that', 'treat')\n",
      "('treat', 'the')\n",
      "('the', 'topic')\n",
      "('topic', 'in')\n",
      "('in', 'substantive')\n",
      "('substantive', 'detail,')\n",
      "('detail,', 'such')\n",
      "('such', 'as')\n",
      "('as', 'books,')\n",
      "('books,', 'newspapers,')\n",
      "('newspapers,', 'magazines,')\n",
      "('magazines,', 'peer-reviewed')\n",
      "('peer-reviewed', 'scholarly')\n",
      "('scholarly', 'journals')\n",
      "('journals', 'and')\n",
      "('and', 'websites')\n",
      "('websites', 'that')\n",
      "('that', 'meet')\n",
      "('meet', 'the')\n",
      "('the', 'same')\n",
      "('same', 'requirements')\n",
      "('requirements', 'as')\n",
      "('as', 'reputable')\n",
      "('reputable', 'print-based')\n",
      "('print-based', 'sources.')\n",
      "('sources.', 'Information')\n",
      "('Information', 'on')\n",
      "('on', 'Wikipedia')\n",
      "('Wikipedia', 'must')\n",
      "('must', 'be')\n",
      "('be', 'verifiable;')\n",
      "('verifiable;', 'if')\n",
      "('if', 'no')\n",
      "('no', 'reliable')\n",
      "('reliable', 'third-party')\n",
      "('third-party', 'sources')\n",
      "('sources', 'can')\n",
      "('can', 'be')\n",
      "('be', 'found')\n",
      "('found', 'on')\n",
      "('on', 'a')\n",
      "('a', 'topic,')\n",
      "('topic,', 'then')\n",
      "('then', 'it')\n",
      "('it', 'should')\n",
      "('should', 'not')\n",
      "('not', 'have')\n",
      "('have', 'a')\n",
      "('a', 'separate')\n",
      "('separate', 'article.')\n",
      "('article.', 'Please')\n",
      "('Please', 'search')\n",
      "('search', 'Wikipedia')\n",
      "('Wikipedia', 'first')\n",
      "('first', 'to')\n",
      "('to', 'make')\n",
      "('make', 'sure')\n",
      "('sure', 'that')\n",
      "('that', 'an')\n",
      "('an', 'article')\n",
      "('article', 'does')\n",
      "('does', 'not')\n",
      "('not', 'already')\n",
      "('already', 'exist')\n",
      "('exist', 'on')\n",
      "('on', 'the')\n",
      "('the', 'subject')\n"
     ]
    }
   ],
   "source": [
    "#Bigrams\n",
    "from nltk import ngrams\n",
    "ng = ngrams(text.split(),2)\n",
    "\n",
    "for n in ng:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32427c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "from string import punctuation\n",
    "punct = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3eefded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lowercase_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f3945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tokens = [token for token in lowercase_tokens if token not in stop_words and token not in punctuation]\n",
    "len(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e9803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "late\n",
      "assess\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('lately'))\n",
    "print(ps.stem('assess'))\n",
    "print(ps.stem('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [ps.stem(token) for token in cleaned_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e8974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shahv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shahv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecde5ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "good\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('starting', 'v'))\n",
    "print(lemmatizer.lemmatize('better', 'a'))\n",
    "print(lemmatizer.lemmatize('run', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ad037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = [lemmatizer.lemmatize for token in cleaned_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c7e8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shahv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('welcome', 'JJ'),\n",
       " ('wikipedia', 'VBD'),\n",
       " ('new', 'JJ'),\n",
       " ('editor', 'NN'),\n",
       " ('become', 'NN'),\n",
       " ('wikipedian', 'JJ'),\n",
       " ('starting', 'VBG'),\n",
       " ('new', 'JJ'),\n",
       " ('article', 'NN'),\n",
       " ('please', 'NN'),\n",
       " ('review', 'VB'),\n",
       " ('wikipedia', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('notability', 'NN'),\n",
       " ('requirements', 'NNS'),\n",
       " ('short', 'JJ'),\n",
       " ('topic', 'NN'),\n",
       " ('article', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('already', 'RB'),\n",
       " ('subject', 'VB'),\n",
       " ('publication', 'NN'),\n",
       " ('reliable', 'JJ'),\n",
       " ('secondary', 'JJ'),\n",
       " ('entirely', 'RB'),\n",
       " ('independent', 'JJ'),\n",
       " ('sources', 'NNS'),\n",
       " ('treat', 'VBP'),\n",
       " ('topic', 'JJ'),\n",
       " ('substantive', 'JJ'),\n",
       " ('detail', 'NN'),\n",
       " ('books', 'NNS'),\n",
       " ('newspapers', 'NNS'),\n",
       " ('magazines', 'NNS'),\n",
       " ('peer-reviewed', 'VBD'),\n",
       " ('scholarly', 'JJ'),\n",
       " ('journals', 'NNS'),\n",
       " ('websites', 'VBZ'),\n",
       " ('meet', 'JJ'),\n",
       " ('requirements', 'NNS'),\n",
       " ('reputable', 'JJ'),\n",
       " ('print-based', 'JJ'),\n",
       " ('sources', 'NNS'),\n",
       " ('information', 'NN'),\n",
       " ('wikipedia', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('verifiable', 'VB'),\n",
       " ('reliable', 'JJ'),\n",
       " ('third-party', 'JJ'),\n",
       " ('sources', 'NNS'),\n",
       " ('found', 'VBD'),\n",
       " ('topic', 'NN'),\n",
       " ('separate', 'JJ'),\n",
       " ('article', 'NN'),\n",
       " ('please', 'NN'),\n",
       " ('search', 'NN'),\n",
       " ('wikipedia', 'NN'),\n",
       " ('first', 'RB'),\n",
       " ('make', 'VB'),\n",
       " ('sure', 'JJ'),\n",
       " ('article', 'NN'),\n",
       " ('already', 'RB'),\n",
       " ('exist', 'VBP'),\n",
       " ('subject', 'JJ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba06f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2023920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ca374f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = nltk.pos_tag(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f93695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('welcome', 'a'), ('wikipedia', 'v'), ('new', 'a'), ('editor', 'n'), ('become', 'n'), ('wikipedian', 'a'), ('starting', 'v'), ('new', 'a'), ('article', 'n'), ('please', 'n'), ('review', 'v'), ('wikipedia', 'n'), (\"'s\", None), ('notability', 'n'), ('requirements', 'n'), ('short', 'a'), ('topic', 'n'), ('article', 'n'), ('must', None), ('already', 'r'), ('subject', 'v'), ('publication', 'n'), ('reliable', 'a'), ('secondary', 'a'), ('entirely', 'r'), ('independent', 'a'), ('sources', 'n'), ('treat', 'v'), ('topic', 'a'), ('substantive', 'a'), ('detail', 'n'), ('books', 'n'), ('newspapers', 'n'), ('magazines', 'n'), ('peer-reviewed', 'v'), ('scholarly', 'a'), ('journals', 'n'), ('websites', 'v'), ('meet', 'a'), ('requirements', 'n'), ('reputable', 'a'), ('print-based', 'a'), ('sources', 'n'), ('information', 'n'), ('wikipedia', 'n'), ('must', None), ('verifiable', 'v'), ('reliable', 'a'), ('third-party', 'a'), ('sources', 'n'), ('found', 'v'), ('topic', 'n'), ('separate', 'a'), ('article', 'n'), ('please', 'n'), ('search', 'n'), ('wikipedia', 'n'), ('first', 'r'), ('make', 'v'), ('sure', 'a'), ('article', 'n'), ('already', 'r'), ('exist', 'v'), ('subject', 'a')]\n"
     ]
    }
   ],
   "source": [
    "#We use our own tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "print(wordnet_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be0f4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for word in wordnet_tagged:\n",
    "    if word[1] != None:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word[0], word[1]))\n",
    "    else:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a68d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'wikipedia',\n",
       " 'new',\n",
       " 'editor',\n",
       " 'become',\n",
       " 'wikipedian',\n",
       " 'start',\n",
       " 'new',\n",
       " 'article',\n",
       " 'please',\n",
       " 'review',\n",
       " 'wikipedia',\n",
       " \"'s\",\n",
       " 'notability',\n",
       " 'requirement',\n",
       " 'short',\n",
       " 'topic',\n",
       " 'article',\n",
       " 'must',\n",
       " 'already',\n",
       " 'subject',\n",
       " 'publication',\n",
       " 'reliable',\n",
       " 'secondary',\n",
       " 'entirely',\n",
       " 'independent',\n",
       " 'source',\n",
       " 'treat',\n",
       " 'topic',\n",
       " 'substantive',\n",
       " 'detail',\n",
       " 'book',\n",
       " 'newspaper',\n",
       " 'magazine',\n",
       " 'peer-reviewed',\n",
       " 'scholarly',\n",
       " 'journal',\n",
       " 'websites',\n",
       " 'meet',\n",
       " 'requirement',\n",
       " 'reputable',\n",
       " 'print-based',\n",
       " 'source',\n",
       " 'information',\n",
       " 'wikipedia',\n",
       " 'must',\n",
       " 'verifiable',\n",
       " 'reliable',\n",
       " 'third-party',\n",
       " 'source',\n",
       " 'find',\n",
       " 'topic',\n",
       " 'separate',\n",
       " 'article',\n",
       " 'please',\n",
       " 'search',\n",
       " 'wikipedia',\n",
       " 'first',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'article',\n",
       " 'already',\n",
       " 'exist',\n",
       " 'subject']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatized\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87576d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   good  job  miss\n",
      "0     1    1     1\n",
      "1     1    0     0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahv\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence_1 = 'This is a good job. I will not miss it for anything.'\n",
    "sentence_2 = 'This is not good at all.'\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range = (1,1), #To use bigrams: ngram_range(2,2)\n",
    "                          stop_words = 'english')\n",
    "\n",
    "#Transform\n",
    "Count_data = CountVec.fit_transform([sentence_1, sentence_2])\n",
    "\n",
    "#Create Dataframe\n",
    "cv_dataframe = pd.DataFrame(Count_data.toarray(), columns = CountVec.get_feature_names())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f8b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        all  anything        at       for      good        is        it  \\\n",
      "0  0.000000  0.353003  0.000000  0.353003  0.251164  0.251164  0.353003   \n",
      "1  0.498446  0.000000  0.498446  0.000000  0.354649  0.354649  0.000000   \n",
      "\n",
      "        job      miss       not      this      will  \n",
      "0  0.353003  0.353003  0.251164  0.251164  0.353003  \n",
      "1  0.000000  0.000000  0.354649  0.354649  0.000000  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahv\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentence_1 = 'This is a good job. I will not miss it for anything.'\n",
    "sentence_2 = 'This is not good at all.'\n",
    "\n",
    "#Without Smooth IDF\n",
    "\n",
    "#Define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf = True, ngram_range = (1,1)) #To use only bigrams: ngram_range = (2,2)\n",
    "\n",
    "#Transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform([sentence_1, sentence_2])\n",
    "\n",
    "#Create Dataframe\n",
    "tf_idf_dataframe = pd.DataFrame(tf_idf_data.toarray(), columns = tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
